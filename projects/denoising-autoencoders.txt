1:"$Sreact.fragment"
2:I[27423,["/_next/static/chunks/53dde413f4b80063.js"],"ThemeProvider"]
3:I[29014,["/_next/static/chunks/53dde413f4b80063.js"],"Sidebar"]
4:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"default"]
5:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"default"]
6:I[22016,["/_next/static/chunks/53dde413f4b80063.js"],""]
8:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"OutletBoundary"]
9:"$Sreact.suspense"
b:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"ViewportBoundary"]
d:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"MetadataBoundary"]
f:I[68027,[],"default"]
:HL["/_next/static/chunks/74d3e83b6b525106.css","style"]
:HL["/_next/static/media/38df7484fe560b25-s.p.60f15535.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"oHSLnI6oTdUg7pIe4m1zz","c":["","projects","denoising-autoencoders"],"q":"","i":false,"f":[[["",{"children":["projects",{"children":[["slug","denoising-autoencoders","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/74d3e83b6b525106.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/53dde413f4b80063.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"inter_fe8b9d92-module__LINzvG__variable libre_baskerville_ad50d965-module__YEzWoG__variable antialiased font-sans bg-background text-foreground","children":["$","$L2",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"children":["$","div",null,{"className":"relative z-10 flex flex-col md:flex-row min-h-screen max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 gap-12","children":[["$","aside",null,{"className":"w-full md:w-64 flex-shrink-0","children":["$","$L3",null,{}]}],["$","main",null,{"className":"flex-1 min-w-0","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"flex flex-col items-start justify-center min-h-[50vh] space-y-4","children":[["$","h1",null,{"className":"text-4xl font-serif font-bold text-tertiary","children":"404"}],["$","h2",null,{"className":"text-xl font-bold","children":"Page Not Found"}],["$","p",null,{"className":"text-muted-foreground","children":"The page you are looking for does not exist. It might have been moved or deleted."}],["$","$L6",null,{"href":"/","className":"text-primary hover:underline","children":"Return Home"}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L7",null,["$","$L8",null,{"children":["$","$9",null,{"name":"Next.MetadataOutlet","children":"$@a"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$Lb",null,{"children":"$Lc"}],["$","div",null,{"hidden":true,"children":["$","$Ld",null,{"children":["$","$9",null,{"name":"Next.Metadata","children":"$Le"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$f",[]],"S":true}
7:["$","article",null,{"className":"max-w-2xl mx-auto md:mx-0","children":[["$","div",null,{"className":"mb-8","children":[["$","$L6",null,{"href":"/projects","className":"inline-flex items-center text-sm text-muted-foreground hover:text-foreground transition-colors mb-6 group","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left mr-2 h-4 w-4 group-hover:-translate-x-1 transition-transform","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],"Back to Projects"]}],["$","header",null,{"className":"border-b border-muted pb-8","children":[["$","h1",null,{"className":"text-3xl md:text-4xl font-serif font-bold mb-4","children":"Denoising Autoencoders"}],["$","div",null,{"className":"flex items-center text-sm text-muted-foreground font-mono","children":[["$","time",null,{"children":"November 2025"}],["$","span",null,{"className":"mx-2","children":"•"}],["$","span",null,{"children":"Project"}]]}]]}]]}],["$","div",null,{"className":"prose prose-neutral dark:prose-invert max-w-none prose-headings:font-serif prose-headings:font-bold prose-h1:text-3xl prose-h1:mb-4 prose-h2:text-2xl prose-h2:mt-8 prose-h2:mb-4 prose-p:leading-relaxed prose-p:mb-4 prose-a:text-primary prose-a:no-underline hover:prose-a:text-accent hover:prose-a:underline prose-blockquote:border-l-primary prose-blockquote:bg-highlight prose-blockquote:py-1 prose-blockquote:px-4 prose-blockquote:rounded-r","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 not-prose mb-6","children":[["$","a",null,{"href":"https://github.com/krishmula/denoising-autoencoders","target":"_blank","className":"inline-flex items-center gap-2 px-3 py-1.5 text-sm rounded-md bg-highlight border border-muted text-foreground hover:border-foreground/30 transition-colors no-underline","children":[["$","svg",null,{"className":"w-4 h-4","fill":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"d":"M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"}]}],["$","p",null,{"children":"GitHub"}]]}],["$","span",null,{"className":"inline-flex items-center gap-2 px-3 py-1.5 text-sm rounded-md border border-dashed border-muted/50 text-muted-foreground/50 cursor-not-allowed","children":[["$","svg",null,{"className":"w-4 h-4","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":[["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":"2","d":"M15 12a3 3 0 11-6 0 3 3 0 016 0z"}],["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":"2","d":"M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"}]]}],["$","p",null,{"children":"Live Demo Coming Soon"}]]}]]}],"\n",["$","p",null,{"children":"Denoising autoencoder that restores corrupted images by learning compressed latent representations through a bottleneck architecture. The system trains on synthetically noised images and learns to reconstruct clean versions, handling both Gaussian noise and more complex noise patterns. Developed as part of SJSU AI/ML Club's Fall 2025 research initiative."}],"\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n"]}]]}]
10:["$","h2",null,{"children":"Encoder-Decoder Architecture"}]
11:["$","p",null,{"children":"The model implements a symmetric convolutional architecture with a compressed latent space. The encoder progressively reduces spatial dimensions through strided convolutions while expanding feature channels, creating information-rich representations at the bottleneck. The decoder mirrors this structure with transposed convolutions, reconstructing full-resolution images while preserving fine-grained details through skip connections between corresponding encoder-decoder layers."}]
12:["$","h2",null,{"children":"Technical Features"}]
13:["$","ul",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Multi-scale Feature Extraction"}]," — Convolutional encoder with downsampling stages capturing hierarchical image features from edges to textures"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Bottleneck Compression"}]," — Reduces input to a compact latent representation, forcing the network to learn efficient noise-invariant encodings"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Skip Connections"}]," — Element-wise concatenation between encoder and decoder feature maps at matching resolutions, preserving spatial information lost during compression"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Adaptive Noise Injection"}]," — Training pipeline generates synthetic noisy images with configurable Gaussian and salt-and-pepper noise parameters"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Combined Loss Function"}]," — MSE + perceptual loss for pixel-wise reconstruction while maintaining visual quality"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Batch Normalization"}]," — Layer normalization after each convolution block for training stability and faster convergence"]}],"\n"]}],"\n"]}]
14:["$","h2",null,{"children":"Architecture"}]
15:["$","p",null,{"children":"The training pipeline loads CIFAR-10 images, applies random noise augmentation, and feeds corrupted-clean pairs to the model. The encoder uses ReLU activations with max pooling for downsampling, while the decoder employs transposed convolutions with nearest-neighbor upsampling. The system tracks PSNR/SSIM/MSE metrics during training and validates on a held-out test set with varying noise intensities."}]
16:["$","h2",null,{"children":"Tech Stack"}]
17:["$","div",null,{"className":"flex flex-wrap gap-2 not-prose","children":[["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"PyTorch"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"NumPy"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"Matplotlib"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"TensorBoard"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"PIL"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"CIFAR-10"}]]}]
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
18:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"IconMark"]
a:null
e:[["$","title","0",{"children":"Denoising Autoencoders | Projects"}],["$","meta","1",{"name":"description","content":"Convolutional autoencoder system for removing Gaussian and non-Gaussian noise from images using encoder-decoder architecture with skip connections."}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L18","3",{}]]
