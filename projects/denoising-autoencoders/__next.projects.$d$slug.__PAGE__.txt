1:"$Sreact.fragment"
2:I[22016,["/_next/static/chunks/53dde413f4b80063.js"],""]
c:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"OutletBoundary"]
d:"$Sreact.suspense"
0:{"buildId":"oHSLnI6oTdUg7pIe4m1zz","rsc":["$","$1","c",{"children":[["$","article",null,{"className":"max-w-2xl mx-auto md:mx-0","children":[["$","div",null,{"className":"mb-8","children":[["$","$L2",null,{"href":"/projects","className":"inline-flex items-center text-sm text-muted-foreground hover:text-foreground transition-colors mb-6 group","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left mr-2 h-4 w-4 group-hover:-translate-x-1 transition-transform","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],"Back to Projects"]}],["$","header",null,{"className":"border-b border-muted pb-8","children":[["$","h1",null,{"className":"text-3xl md:text-4xl font-serif font-bold mb-4","children":"Denoising Autoencoders"}],["$","div",null,{"className":"flex items-center text-sm text-muted-foreground font-mono","children":[["$","time",null,{"children":"November 2025"}],["$","span",null,{"className":"mx-2","children":"•"}],["$","span",null,{"children":"Project"}]]}]]}]]}],["$","div",null,{"className":"prose prose-neutral dark:prose-invert max-w-none prose-headings:font-serif prose-headings:font-bold prose-h1:text-3xl prose-h1:mb-4 prose-h2:text-2xl prose-h2:mt-8 prose-h2:mb-4 prose-p:leading-relaxed prose-p:mb-4 prose-a:text-primary prose-a:no-underline hover:prose-a:text-accent hover:prose-a:underline prose-blockquote:border-l-primary prose-blockquote:bg-highlight prose-blockquote:py-1 prose-blockquote:px-4 prose-blockquote:rounded-r","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 not-prose mb-6","children":[["$","a",null,{"href":"https://github.com/krishmula/denoising-autoencoders","target":"_blank","className":"inline-flex items-center gap-2 px-3 py-1.5 text-sm rounded-md bg-highlight border border-muted text-foreground hover:border-foreground/30 transition-colors no-underline","children":[["$","svg",null,{"className":"w-4 h-4","fill":"currentColor","viewBox":"0 0 24 24","children":["$","path",null,{"d":"M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"}]}],["$","p",null,{"children":"GitHub"}]]}],["$","span",null,{"className":"inline-flex items-center gap-2 px-3 py-1.5 text-sm rounded-md border border-dashed border-muted/50 text-muted-foreground/50 cursor-not-allowed","children":[["$","svg",null,{"className":"w-4 h-4","fill":"none","stroke":"currentColor","viewBox":"0 0 24 24","children":[["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":"2","d":"M15 12a3 3 0 11-6 0 3 3 0 016 0z"}],["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","strokeWidth":"2","d":"M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"}]]}],["$","p",null,{"children":"Live Demo Coming Soon"}]]}]]}],"\n",["$","p",null,{"children":"Denoising autoencoder that restores corrupted images by learning compressed latent representations through a bottleneck architecture. The system trains on synthetically noised images and learns to reconstruct clean versions, handling both Gaussian noise and more complex noise patterns. Developed as part of SJSU AI/ML Club's Fall 2025 research initiative."}],"\n","$L3","\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n"]}]]}],null,"$Lb"]}],"loading":null,"isPartial":false}
3:["$","h2",null,{"children":"Encoder-Decoder Architecture"}]
4:["$","p",null,{"children":"The model implements a symmetric convolutional architecture with a compressed latent space. The encoder progressively reduces spatial dimensions through strided convolutions while expanding feature channels, creating information-rich representations at the bottleneck. The decoder mirrors this structure with transposed convolutions, reconstructing full-resolution images while preserving fine-grained details through skip connections between corresponding encoder-decoder layers."}]
5:["$","h2",null,{"children":"Technical Features"}]
6:["$","ul",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Multi-scale Feature Extraction"}]," — Convolutional encoder with downsampling stages capturing hierarchical image features from edges to textures"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Bottleneck Compression"}]," — Reduces input to a compact latent representation, forcing the network to learn efficient noise-invariant encodings"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Skip Connections"}]," — Element-wise concatenation between encoder and decoder feature maps at matching resolutions, preserving spatial information lost during compression"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Adaptive Noise Injection"}]," — Training pipeline generates synthetic noisy images with configurable Gaussian and salt-and-pepper noise parameters"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Combined Loss Function"}]," — MSE + perceptual loss for pixel-wise reconstruction while maintaining visual quality"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Batch Normalization"}]," — Layer normalization after each convolution block for training stability and faster convergence"]}],"\n"]}],"\n"]}]
7:["$","h2",null,{"children":"Architecture"}]
8:["$","p",null,{"children":"The training pipeline loads CIFAR-10 images, applies random noise augmentation, and feeds corrupted-clean pairs to the model. The encoder uses ReLU activations with max pooling for downsampling, while the decoder employs transposed convolutions with nearest-neighbor upsampling. The system tracks PSNR/SSIM/MSE metrics during training and validates on a held-out test set with varying noise intensities."}]
9:["$","h2",null,{"children":"Tech Stack"}]
a:["$","div",null,{"className":"flex flex-wrap gap-2 not-prose","children":[["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"PyTorch"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"NumPy"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"Matplotlib"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"TensorBoard"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"PIL"}],["$","span",null,{"className":"px-3 py-1 text-sm rounded-full bg-highlight text-foreground border border-muted","children":"CIFAR-10"}]]}]
b:["$","$Lc",null,{"children":["$","$d",null,{"name":"Next.MetadataOutlet","children":"$@e"}]}]
e:null
